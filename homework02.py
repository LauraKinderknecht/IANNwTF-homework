# -*- coding: utf-8 -*-
"""Untitled23.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eyfM8oBYHoiISa9Pe-gxx0DQbGn1IaK-
"""

import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt

#2.1 Loading the MNIST dataset
( train_dataset , test_dataset ) , ds_info = tfds.load( "mnist" , split =[ "train" ,"test"] , as_supervised = True , with_info = True )
"""
print(ds_info)
How many training/test images are there? 60000/ 10000
What's the image shape? (28,28,1)
What range are pixel values in? data type = uint8, range =

tfds.show_examples(train_dataset, ds_info)
"""

#2.2 Setting up the data pipeline
def preprocessing_data(mnist):
    #flatten images to 28x28
    mnist = mnist.map(lambda img, target: (tf.reshape(img, (-1,)), target))
    #change datatype of images
    mnist = mnist.map(lambda img, target: (tf.cast(img, tf.float32),target))
    #normalize image values
    mnist = mnist.map(lambda img, target:((img/128.)-1.,target))
    #one-hot encode labels
    mnist = mnist.map(lambda img, target: (img, tf.one_hot(target, depth = 10)))
    #cache progress
    mnist = mnist.cache()
    #shuffle, batch, prefetch
    mnist = mnist.shuffle(1000)
    mnist = mnist.batch(32) #b.  mnist = mnist.batch(64) (batch size adjustment)
    mnist = mnist.prefetch(20)
    return mnist

#apply preprocessing
train_ds = train_dataset.apply(preprocessing_data)
test_ds = test_dataset.apply(preprocessing_data)

##2.3 Building a deep neural network with TensorFlow
#create empty network
model = tf.keras.models.Sequential([

    #add input layer that flattens input shape 28x28 to 783 units in a flat line
    #tf.keras.layers.Flatten(input_shape=(28,28)),

    #add 2 hidden layers with 256 units each and relu activation function
    tf.keras.layers.Dense(units=256, activation=tf.nn.relu),
    tf.keras.layers.Dense(units=256, activation=tf.nn.relu),       #c. tf.keras.layers.Dense(units=128, activation=of.nn.reul) (size of unit & number of layer adjustment)


    #add output layer with 10 units (one for each possible answer (0-10))
    #softmax activation function gives probability for every possible output
    tf.keras.layers.Dense(units=10, activation=tf.nn.softmax)])

#see model summary
#model.summary()

##2.4 Training the Network
#Hyperparameters
num_epochs = 10
lr = 0.1   #a. lr = 0.01(learning rate adjustment)


#initialize loss function
cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()
#initialize optimizer
optimizer = tf.keras.optimizers.SGD(learning_rate=lr) #d. optimizer = tf.keras.optimizers.Adam(learning_rate=lr) (optimizer adjustment)

#initializations for visualization
train_losses = []
test_losses = []
test_accuracies = []

#training step function
def train_step(model, input, target, loss_func, optimizer):
    with tf.GradientTape() as tape:
        prediction = model(input)
        loss = loss_func(target, prediction)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss

#test model function
def test(model, test_data, loss_function):

    test_accuracy_aggr = []
    test_loss_aggr = []

    for (input, target) in test_data:
        prediction = model(input)
        sample_test_loss = loss_function(target, prediction)
        sample_test_accuracy = np.argmax(target, axis=1) == np.argmax(prediction, axis=1)
        sample_test_accuracy = np.mean(sample_test_accuracy)
        test_loss_aggr.append(sample_test_loss.numpy())
        test_accuracy_aggr.append(np.mean(sample_test_accuracy))

    test_loss = tf.reduce_mean(test_loss_aggr)
    test_accuracy = tf.reduce_mean(test_accuracy_aggr)

    return test_loss, test_accuracy


#training loop function
def training_loop(num_epochs, model, train_ds, test_ds, loss, optimizer, train_losses, test_losses, test_accuracies):
    for epoch in range(num_epochs):
        print(f"Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}")

        epoch_loss_aggr = []
        for input, target in train_ds:
            train_loss = train_step(model, input, target, loss, optimizer)
            epoch_loss_aggr.append(train_loss)

        train_losses.append(tf.reduce_mean(epoch_loss_aggr))

        test_loss, test_accuracy = test(model, test_ds, loss)
        test_losses.append(test_loss)
        test_accuracies.append(test_accuracy)

    return train_losses, test_losses,  test_accuracies

#test
test_loss, test_accuracy = test(model, test_ds, cross_entropy_loss)
test_losses.append(test_loss)
test_accuracies.append(test_accuracy)

#check performance
train_loss, _ = test(model, train_ds, cross_entropy_loss)
train_losses.append(train_loss)

#train the model
train_losses, test_losses, test_accuracies  = training_loop(num_epochs, model, train_ds, test_ds, cross_entropy_loss, optimizer, train_losses, test_losses, test_accuracies)

##2.5 Visualization
def visualization(train_losses , test_losses ,test_accuracies ):
    plt.figure ()
    line1 , = plt.plot( train_losses , "b-")
    line2 , = plt.plot( test_losses , "r-")
    line3 , = plt.plot ( test_accuracies , "r:")
    plt.xlabel("Training steps")
    plt.ylabel("Loss / Accuracy")
    plt.legend(( line1 , line2 , line3 ), ("training loss", "test loss", "test accuracy"))
    plt.show()

visualization(train_losses, test_losses, test_accuracies)

##3.0 Adjusting hyperparameters(commented in the codes above)
'''

a. Learning rate
Modified the learning rate (lr) from 0.1 to 0.01.
-> A high learning rate may cause the model to overshoot the minimum, leading to slower convergence or even divergence.
   A lower learning rate may allow the model to converge more gradually, potentially improving performance.


b. Batch size
Adjusted the batch size from 32 to 64
-> Batch size affects how many samples are processed before updating the model.
   Smaller batch sizes may provide a regularization effect and might be beneficial for generalization.
   Larger batch sizes may result in faster training, but they could also require more memory.


c. Model Architecture
Added or remove hidden layers and/or adjust the number of units in each layer.
-> A more complex model with additional layers or units may capture intricate patterns in the data, but it could also lead to overfitting.
   On the other hand, a simpler model may generalize better but might struggle with complex patterns.

d. Optimizer
Replaced the SGD optimizer with Adam.
-> Optimizers influence how the model's weights are updated. Different optimizers have unique characteristics and may perform better in specific scenarios.
   Adam is known for combining the benefits of both AdaGrad and RMSProp, and it often converges faster than plain SGD.

'''
